{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "Read train images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n",
      "Read train data time: 86.13 seconds\n",
      "Unique drivers: 26\n",
      "['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n",
      "('Train shape:', (22424, 1, 64, 64))\n",
      "(22424, 'train samples')\n",
      "Read test images\n",
      "Read 7972 images from 79726\n",
      "Read 15944 images from 79726\n",
      "Read 23916 images from 79726\n",
      "Read 31888 images from 79726\n",
      "Read 39860 images from 79726\n",
      "Read 47832 images from 79726\n",
      "Read 55804 images from 79726\n",
      "Read 63776 images from 79726\n",
      "Read 71748 images from 79726\n",
      "Read 79720 images from 79726\n",
      "Read test data time: 305.8 seconds\n",
      "('Test shape:', (79726, 1, 64, 64))\n",
      "(79726, 'test samples')\n",
      "Start KFold number 1 from 13\n",
      "('Split train: ', 20547, 20547)\n",
      "('Split valid: ', 1877, 1877)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p026', 'p035', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p024', 'p039'])\n",
      "Train on 20547 samples, validate on 1877 samples\n",
      "Epoch 1/50\n",
      "20547/20547 [==============================] - 17s - loss: 2.6775 - val_loss: 2.0445\n",
      "Epoch 2/50\n",
      "20547/20547 [==============================] - 17s - loss: 1.8705 - val_loss: 1.5557\n",
      "Epoch 3/50\n",
      "20547/20547 [==============================] - 17s - loss: 1.4893 - val_loss: 1.2443\n",
      "Epoch 4/50\n",
      "20547/20547 [==============================] - 17s - loss: 1.2239 - val_loss: 1.0326\n",
      "Epoch 5/50\n",
      "20547/20547 [==============================] - 17s - loss: 1.0638 - val_loss: 1.1138\n",
      "Epoch 6/50\n",
      "20547/20547 [==============================] - 17s - loss: 0.9432 - val_loss: 1.0175\n",
      "Epoch 7/50\n",
      "20547/20547 [==============================] - 17s - loss: 0.8661 - val_loss: 0.8566\n",
      "Epoch 8/50\n",
      "20547/20547 [==============================] - 17s - loss: 0.8215 - val_loss: 0.8599\n",
      "Epoch 9/50\n",
      "20547/20547 [==============================] - 17s - loss: 0.7701 - val_loss: 0.9669\n",
      "1877/1877 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.85661232385341279)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 2 from 13\n",
      "('Split train: ', 20882, 20882)\n",
      "('Split valid: ', 1542, 1542)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p026', 'p072'])\n",
      "Train on 20882 samples, validate on 1542 samples\n",
      "Epoch 1/50\n",
      "20882/20882 [==============================] - 17s - loss: 0.8327 - val_loss: 0.5343\n",
      "Epoch 2/50\n",
      "20882/20882 [==============================] - 17s - loss: 0.7701 - val_loss: 0.7270\n",
      "Epoch 3/50\n",
      "20882/20882 [==============================] - 17s - loss: 0.7319 - val_loss: 0.9148\n",
      "1542/1542 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.53428671473735689)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 3 from 13\n",
      "('Split train: ', 20792, 20792)\n",
      "('Split valid: ', 1632, 1632)\n",
      "('Train drivers: ', ['p002', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p012', 'p061'])\n",
      "Train on 20792 samples, validate on 1632 samples\n",
      "Epoch 1/50\n",
      "20792/20792 [==============================] - 17s - loss: 0.7815 - val_loss: 0.5569\n",
      "Epoch 2/50\n",
      "20792/20792 [==============================] - 17s - loss: 0.7416 - val_loss: 0.7061\n",
      "Epoch 3/50\n",
      "20792/20792 [==============================] - 17s - loss: 0.7063 - val_loss: 0.6475\n",
      "1632/1632 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.55686217526960513)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 4 from 13\n",
      "('Split train: ', 20754, 20754)\n",
      "('Split valid: ', 1670, 1670)\n",
      "('Train drivers: ', ['p002', 'p012', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p014', 'p056'])\n",
      "Train on 20754 samples, validate on 1670 samples\n",
      "Epoch 1/50\n",
      "20754/20754 [==============================] - 17s - loss: 0.7397 - val_loss: 0.7167\n",
      "Epoch 2/50\n",
      "20754/20754 [==============================] - 17s - loss: 0.7144 - val_loss: 0.7232\n",
      "Epoch 3/50\n",
      "20754/20754 [==============================] - 17s - loss: 0.6885 - val_loss: 0.8635\n",
      "1670/1670 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.71669603833548579)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 5 from 13\n",
      "('Split train: ', 20586, 20586)\n",
      "('Split valid: ', 1838, 1838)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p024', 'p026', 'p035', 'p039', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p022', 'p041'])\n",
      "Train on 20586 samples, validate on 1838 samples\n",
      "Epoch 1/50\n",
      "20586/20586 [==============================] - 17s - loss: 0.7426 - val_loss: 0.2297\n",
      "Epoch 2/50\n",
      "20586/20586 [==============================] - 17s - loss: 0.7165 - val_loss: 0.3157\n",
      "Epoch 3/50\n",
      "20586/20586 [==============================] - 17s - loss: 0.6820 - val_loss: 0.4681\n",
      "1838/1838 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.22969271341851102)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 6 from 13\n",
      "('Split train: ', 20825, 20825)\n",
      "('Split valid: ', 1599, 1599)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p015', 'p045'])\n",
      "Train on 20825 samples, validate on 1599 samples\n",
      "Epoch 1/50\n",
      "20825/20825 [==============================] - 17s - loss: 0.7036 - val_loss: 0.5023\n",
      "Epoch 2/50\n",
      "20825/20825 [==============================] - 17s - loss: 0.6831 - val_loss: 0.4786\n",
      "Epoch 3/50\n",
      "20825/20825 [==============================] - 17s - loss: 0.6561 - val_loss: 0.5790\n",
      "Epoch 4/50\n",
      "20825/20825 [==============================] - 18s - loss: 0.6432 - val_loss: 0.6374\n",
      "1599/1599 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.47860428157694612)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 7 from 13\n",
      "('Split train: ', 20335, 20335)\n",
      "('Split valid: ', 2089, 2089)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p016', 'p049'])\n",
      "Train on 20335 samples, validate on 2089 samples\n",
      "Epoch 1/50\n",
      "20335/20335 [==============================] - 17s - loss: 0.6732 - val_loss: 0.1931\n",
      "Epoch 2/50\n",
      "20335/20335 [==============================] - 17s - loss: 0.6440 - val_loss: 0.1848\n",
      "Epoch 3/50\n",
      "20335/20335 [==============================] - 17s - loss: 0.6468 - val_loss: 0.2795\n",
      "Epoch 4/50\n",
      "20335/20335 [==============================] - 17s - loss: 0.6278 - val_loss: 0.3188\n",
      "2089/2089 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.18479703286133659)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 8 from 13\n",
      "('Split train: ', 20913, 20913)\n",
      "('Split valid: ', 1511, 1511)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p045', 'p047', 'p049', 'p050', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p042', 'p051'])\n",
      "Train on 20913 samples, validate on 1511 samples\n",
      "Epoch 1/50\n",
      "20913/20913 [==============================] - 17s - loss: 0.6616 - val_loss: 0.3071\n",
      "Epoch 2/50\n",
      "20913/20913 [==============================] - 18s - loss: 0.6349 - val_loss: 0.3100\n",
      "Epoch 3/50\n",
      "20913/20913 [==============================] - 18s - loss: 0.6254 - val_loss: 0.3842\n",
      "1511/1511 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.30706020190425209)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 9 from 13\n",
      "('Split train: ', 20849, 20849)\n",
      "('Split valid: ', 1575, 1575)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p049', 'p050', 'p051', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p047', 'p052'])\n",
      "Train on 20849 samples, validate on 1575 samples\n",
      "Epoch 1/50\n",
      "20849/20849 [==============================] - 17s - loss: 0.6369 - val_loss: 0.3302\n",
      "Epoch 2/50\n",
      "20849/20849 [==============================] - 17s - loss: 0.6004 - val_loss: 0.3183\n",
      "Epoch 3/50\n",
      "20849/20849 [==============================] - 17s - loss: 0.6046 - val_loss: 0.3459\n",
      "Epoch 4/50\n",
      "20849/20849 [==============================] - 17s - loss: 0.5875 - val_loss: 0.4712\n",
      "1575/1575 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.31828534655822832)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 10 from 13\n",
      "('Split train: ', 20570, 20570)\n",
      "('Split valid: ', 1854, 1854)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p064', 'p066'])\n",
      "Train on 20570 samples, validate on 1854 samples\n",
      "Epoch 1/50\n",
      "20570/20570 [==============================] - 17s - loss: 0.5805 - val_loss: 0.5628\n",
      "Epoch 2/50\n",
      "20570/20570 [==============================] - 17s - loss: 0.5717 - val_loss: 0.8988\n",
      "Epoch 3/50\n",
      "20570/20570 [==============================] - 17s - loss: 0.5434 - val_loss: 0.9624\n",
      "1854/1854 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.56275286912513123)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 11 from 13\n",
      "('Split train: ', 20820, 20820)\n",
      "('Split valid: ', 1604, 1604)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p081'])\n",
      "('Test drivers: ', ['p050', 'p075'])\n",
      "Train on 20820 samples, validate on 1604 samples\n",
      "Epoch 1/50\n",
      "20820/20820 [==============================] - 17s - loss: 0.5909 - val_loss: 0.5867\n",
      "Epoch 2/50\n",
      "20820/20820 [==============================] - 17s - loss: 0.5752 - val_loss: 0.4571\n",
      "Epoch 3/50\n",
      "20820/20820 [==============================] - 17s - loss: 0.5532 - val_loss: 0.6377\n",
      "Epoch 4/50\n",
      "20820/20820 [==============================] - 17s - loss: 0.5404 - val_loss: 0.8850\n",
      "1604/1604 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.4570788890250212)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 12 from 13\n",
      "('Split train: ', 20851, 20851)\n",
      "('Split valid: ', 1573, 1573)\n",
      "('Train drivers: ', ['p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081'])\n",
      "('Test drivers: ', ['p002', 'p035'])\n",
      "Train on 20851 samples, validate on 1573 samples\n",
      "Epoch 1/50\n",
      "20851/20851 [==============================] - 17s - loss: 0.5965 - val_loss: 0.1374\n",
      "Epoch 2/50\n",
      "20851/20851 [==============================] - 17s - loss: 0.5813 - val_loss: 0.2369\n",
      "Epoch 3/50\n",
      "20851/20851 [==============================] - 17s - loss: 0.5694 - val_loss: 0.2267\n",
      "1573/1573 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.13739093611608763)\n",
      "79726/79726 [==============================] - 22s    \n",
      "Start KFold number 13 from 13\n",
      "('Split train: ', 20364, 20364)\n",
      "('Split valid: ', 2060, 2060)\n",
      "('Train drivers: ', ['p002', 'p012', 'p014', 'p015', 'p016', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075'])\n",
      "('Test drivers: ', ['p021', 'p081'])\n",
      "Train on 20364 samples, validate on 2060 samples\n",
      "Epoch 1/50\n",
      "20364/20364 [==============================] - 17s - loss: 0.5730 - val_loss: 0.3522\n",
      "Epoch 2/50\n",
      "20364/20364 [==============================] - 17s - loss: 0.5524 - val_loss: 0.5558\n",
      "Epoch 3/50\n",
      "20364/20364 [==============================] - 17s - loss: 0.5468 - val_loss: 0.6883\n",
      "2060/2060 [==============================] - 0s     \n",
      "('Score log_loss: ', 0.35219832613831953)\n",
      "79726/79726 [==============================] - 22s    \n",
      "('Log_loss train independent avg: ', 0.43677879026161315)\n",
      "Final log_loss: 0.436778790262, rows: 64 cols: 64 nfolds: 13 epoch: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18bbc2dd821a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m     \u001b[0mrun_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m     \u001b[1;31m# run_single()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-18bbc2dd821a>\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[1;34m(nfolds)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;31m# test_res = merge_several_folds_geom(yfull_test, nfolds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[0mcreate_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m     \u001b[0msave_useful_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-18bbc2dd821a>\u001b[0m in \u001b[0;36msave_useful_data\u001b[1;34m(predictions_valid, valid_ids, model, info)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# Save code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[0mcur_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mcode_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'subm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_code.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mcopy2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import time\n",
    "from shutil import copy2\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.misc import imread, imresize, imshow\n",
    "\n",
    "use_cache = 1\n",
    "\n",
    "\n",
    "def show_image(im, name='image'):\n",
    "    cv2.imshow(name, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# color_type = 1 - gray\n",
    "# color_type = 3 - RGB\n",
    "def get_im_cv2(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_im_cv2_mod(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    else:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    rotate = random.uniform(-10, 10)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), rotate, 1)\n",
    "    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    clss = dict()\n",
    "    path = os.path.join('/data', 'driver_imgs_list.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "        if arr[0] not in clss.keys():\n",
    "            clss[arr[0]] = [(arr[1], arr[2])]\n",
    "        else:\n",
    "            clss[arr[0]].append((arr[1], arr[2]))\n",
    "    f.close()\n",
    "    return dr, clss\n",
    "\n",
    "\n",
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    driver_id = []\n",
    "    start_time = time.time()\n",
    "    driver_data, dr_class = get_driver_data()\n",
    "\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('/data', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            # img = get_im_cv2(fl, img_rows, img_cols, color_type)\n",
    "            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(j)\n",
    "            driver_id.append(driver_data[flbase])\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    print(unique_drivers)\n",
    "    return X_train, y_train, X_train_id, driver_id, unique_drivers\n",
    "\n",
    "\n",
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    path = os.path.join('/data', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        # img = get_im_cv2(fl, img_rows, img_cols, color_type)\n",
    "        img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n",
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_model(model, arch_path, weights_path):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(arch_path, 'w').write(json_string)\n",
    "    model.save_weights(weights_path, overwrite=True)\n",
    "\n",
    "\n",
    "def read_model(arch_path, weights_path):\n",
    "    model = model_from_json(open(arch_path).read())\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)\n",
    "\n",
    "\n",
    "def save_useful_data(predictions_valid, valid_ids, model, info):\n",
    "    result1 = pd.DataFrame(predictions_valid, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(valid_ids, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir(os.path.join('subm', 'data')):\n",
    "        os.mkdir(os.path.join('subm', 'data'))\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    # Save predictions\n",
    "    pred_file = os.path.join('subm', 'data', 's_' + suffix + '_train_predictions.csv')\n",
    "    result1.to_csv(pred_file, index=False)\n",
    "    # Save model\n",
    "    json_string = model.to_json()\n",
    "    model_file = os.path.join('subm', 'data', 's_' + suffix + '_model.json')\n",
    "    open(model_file, 'w').write(json_string)\n",
    "    # Save code\n",
    "    cur_code = os.path.realpath(__file__)\n",
    "    code_file = os.path.join('subm', 'data', 's_' + suffix + '_code.py')\n",
    "    copy2(cur_code, code_file)\n",
    "\n",
    "\n",
    "def read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '_rotated.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target, train_id, driver_id, unique_drivers = load_train(img_rows, img_cols, color_type)\n",
    "        cache_data((train_data, train_target, train_id, driver_id, unique_drivers), cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target, train_id, driver_id, unique_drivers) = restore_data(cache_path)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    if color_type == 1:\n",
    "        train_data = train_data.reshape(train_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    train_target = np_utils.to_categorical(train_target, 10)\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data /= 255\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id, driver_id, unique_drivers\n",
    "\n",
    "\n",
    "def read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '_rotated.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "        cache_data((test_data, test_id), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    if color_type == 1:\n",
    "        test_data = test_data.reshape(test_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        test_data = test_data.transpose((0, 3, 1, 2))\n",
    "    # test_data = test_data.swapaxes(3, 1)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id\n",
    "\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def merge_several_folds_geom(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a *= np.array(data[i])\n",
    "    a = np.power(a, 1/nfolds)\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "    for i in range(len(driver_id)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "            index.append(i)\n",
    "    data = np.array(data)\n",
    "    target = np.array(target)\n",
    "    index = np.array(index)\n",
    "    return data, target, index\n",
    "\n",
    "\n",
    "def create_model_v1(img_rows, img_cols, color_type=1):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', init='he_normal',\n",
    "                            input_shape=(color_type, img_rows, img_cols)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(Adam(lr=1e-3), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    return pv\n",
    "\n",
    "\n",
    "def run_cross_validation(nfolds=10):\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    # color type: 1 - grey, 3 - rgb\n",
    "    color_type_global = 1\n",
    "    batch_size = 16\n",
    "    nb_epoch = 50\n",
    "    random_state = 51\n",
    "    restore_from_last_checkpoint = 0\n",
    "\n",
    "    train_data, train_target, train_id, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n",
    "    model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = []\n",
    "    kf = KFold(len(unique_drivers), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    for train_drivers, test_drivers in kf:\n",
    "        unique_list_train = [unique_drivers[i] for i in train_drivers]\n",
    "        X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n",
    "        unique_list_valid = [unique_drivers[i] for i in test_drivers]\n",
    "        X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "        print('Train drivers: ', unique_list_train)\n",
    "        print('Test drivers: ', unique_list_valid)\n",
    "\n",
    "        kfold_weights_path = os.path.join('cache', 'weights_kfold_' + str(num_fold) + '.h5')\n",
    "        if not os.path.isfile(kfold_weights_path) or restore_from_last_checkpoint == 0:\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=1, verbose=0),\n",
    "                ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "            ]\n",
    "            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                  shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n",
    "                  callbacks=callbacks)\n",
    "        if os.path.isfile(kfold_weights_path):\n",
    "            model.load_weights(kfold_weights_path)\n",
    "\n",
    "        # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "        # print('Score log_loss: ', score[0])\n",
    "\n",
    "        predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "        sum_score += score*len(test_index)\n",
    "\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "        # Store test predictions\n",
    "        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    score = sum_score/len(train_data)\n",
    "    print(\"Log_loss train independent avg: \", score)\n",
    "\n",
    "    predictions_valid = get_validation_predictions(train_data, yfull_train)\n",
    "    score1 = log_loss(train_target, predictions_valid)\n",
    "    if abs(score1 - score) > 0.0001:\n",
    "        print('Check error: {} != {}'.format(score, score1))\n",
    "\n",
    "    print('Final log_loss: {}, rows: {} cols: {} nfolds: {} epoch: {}'.format(score, img_rows, img_cols, nfolds, nb_epoch))\n",
    "    info_string = 'loss_' + str(score) \\\n",
    "                    + '_r_' + str(img_rows) \\\n",
    "                    + '_c_' + str(img_cols) \\\n",
    "                    + '_folds_' + str(nfolds) \\\n",
    "                    + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    # test_res = merge_several_folds_geom(yfull_test, nfolds)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "    save_useful_data(predictions_valid, train_id, model, info_string)\n",
    "\n",
    "\n",
    "def run_single():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    color_type_global = 1\n",
    "    batch_size = 32\n",
    "    nb_epoch = 50\n",
    "    random_state = 51\n",
    "\n",
    "    train_data, train_target, train_id, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n",
    "\n",
    "    yfull_test = []\n",
    "    unique_list_train = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p035', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p075', 'p081']\n",
    "    X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n",
    "    unique_list_valid = ['p024', 'p026', 'p039', 'p072']\n",
    "    X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "    print('Start Single Run')\n",
    "    print('Split train: ', len(X_train))\n",
    "    print('Split valid: ', len(X_valid))\n",
    "    print('Train drivers: ', unique_list_train)\n",
    "    print('Valid drivers: ', unique_list_valid)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ]\n",
    "    model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "    # print('Score log_loss: ', score[0])\n",
    "\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "    score = log_loss(Y_valid, predictions_valid)\n",
    "    print('Score log_loss: ', score)\n",
    "\n",
    "    # Store test predictions\n",
    "    test_prediction = model.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "    yfull_test.append(test_prediction)\n",
    "\n",
    "    print('Final log_loss: {}, rows: {} cols: {} epoch: {}'.format(score, img_rows, img_cols, nb_epoch))\n",
    "    info_string = 'loss_' + str(score) \\\n",
    "                    + '_r_' + str(img_rows) \\\n",
    "                    + '_c_' + str(img_cols) \\\n",
    "                    + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    full_pred = model.predict(train_data, batch_size=batch_size, verbose=1)\n",
    "    score = log_loss(train_target, full_pred)\n",
    "    print('Full score log_loss: ', score)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, 1)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "    save_useful_data(full_pred, train_id, model, info_string)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_cross_validation(13)\n",
    "    # run_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
